\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Ensemble Learning Models for CTR Prediction}


\author{
Sampoorna Biswas \\
%\thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.} \\
Department of Computer Science\\
The University of British Columbia\\
\texttt{sbiswas@cs.ubc.ca} \\
\And
Sam Creed \\
Department of Computer Science\\
The University of British Columbia\\
\texttt{samcreed@cs.ubc.ca} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction}

In this section we will

\begin{itemize}
\item state the problem being addressed (CTR prediction)
\item define CTR prediction
\item explain what a kaggle competition is and how it is relevant to this project
\item motivate why this is an important problem
\item highlight challenges of limiting approaches
\item summarize contribution of work/why and what we want to do
\end{itemize}

\section{Related Work}
\label{gen_inst}

In this section we will

\begin{itemize}
\item identify existing research in this area (at least three sources)
% http://research.google.com/pubs/pub41159.html
% http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf -- Google's basic approach
% http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/ -- vowpal wabbit, good for ensembles?
% http://research.microsoft.com/pubs/122779/AdPredictor%20ICML%202010%20-%20final.pdf -- Microsoft's Bing approach (slightly older)
% http://www.commendo.at/UserFiles/commendo/File/KDDCup2012-Track2.pdf -- Opera paper on CF approach, huge mem costs. good presentation, ensemble methods using NN and others. maximizing ROC
\item discuss how our approach differs from existing research or is incomplete (perhaps does not use ensemble methods, or not for this particular application)
\item 

\end{itemize}

\section{Ensemble Learning}

In this section we will give an overview of the different learning stubs used in our ensemble learning algorithm.

\subsection{Ensemble Learning}

We should also perhaps explain what ensemble learning is.

\subsubsection{Stub 1: Naive Bayes?}

Description.

\subsubsection{Stub 2: Random Forest?}

Description.

\subsubsection{Stub 3: Neural Network?}

Description.

\subsubsection{Stub 4: SVM?}

Description.

\subsubsection{Google Fast Solution Approach}

Description. Discuss the fast python solution, maybe use as part of an ensemble, and compare our implementation against it.

\subsection{Dataset and Feature Selection} % split into different sections?

Description. For kaggle competition, discuss train/test data given.

Discuss analysis of features and how we attempt to limit or ignore certain features in the learning process. Maybe something else to compare against with time/quality performance.

\section{Experimental Results}

In this section we should talk about

\begin{itemize}
\item the kaggle competition and the train and test datasets
\item comparison in quality of ensemble, fast solution, stubs, and using feature selection
\item could possibly compare different loss functions for fun - logistic vs hinge loss.
\end{itemize}

\section{Discussion and Future Work}

State main conclusion obtained from the course project. List one strength and one weakness of our contribution. State what we would do with more time.

\subsubsection*{References}

References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point) 
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
